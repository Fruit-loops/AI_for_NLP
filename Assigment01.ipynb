{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T08:05:54.352490Z",
     "start_time": "2019-04-06T08:05:54.312514Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "import jieba\n",
    "import random\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T08:05:54.615339Z",
     "start_time": "2019-04-06T08:05:54.355489Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "fail = [True, None]\n",
    "nonsense = \"Sorry, I don't know what you're  talking about.\"\n",
    "\n",
    "def is_variable(pat):\n",
    "    return pat.startswith('?') and all(s.isalpha() for s in pat[1:])\n",
    "\n",
    "\n",
    "def pat_match(pattern, saying):\n",
    "    if is_variable(pattern[0]): return True\n",
    "    else:\n",
    "        if pattern[0] != saying[0]: return False\n",
    "        else:\n",
    "            return pat_match(pattern[1:], saying[1:])\n",
    "\n",
    "        \n",
    "def pat_match(pattern, saying):\n",
    "    if not pattern or not saying: return []\n",
    "    \n",
    "    if is_variable(pattern[0]):\n",
    "        return [(pattern[0], saying[0])] + pat_match(pattern[1:], saying[1:])\n",
    "    else:\n",
    "        if pattern[0] != saying[0]: return []\n",
    "        else:\n",
    "            return pat_match(pattern[1:], saying[1:])\n",
    "\n",
    "def subsitite(rule, parsed_rules):\n",
    "    if not rule: return []\n",
    "    \n",
    "    return [parsed_rules.get(rule[0], rule[0])] + subsitite(rule[1:], parsed_rules)\n",
    "\n",
    "def is_pattern_segment(pattern):\n",
    "    return pattern.startswith('?*') and all(a.isalpha() for a in pattern[2:])\n",
    "\n",
    "\n",
    "\n",
    "def pat_match_with_seg(pattern, saying):\n",
    "    if not pattern or not saying: return []\n",
    "    \n",
    "    pat = pattern[0]\n",
    "    \n",
    "    if is_variable(pat):\n",
    "        return [(pat, saying[0])] + pat_match_with_seg(pattern[1:], saying[1:])\n",
    "    elif is_pattern_segment(pat):\n",
    "        match, index = segment_match(pattern, saying)\n",
    "        return [match] + pat_match_with_seg(pattern[1:], saying[index:])\n",
    "    elif pat == saying[0]:\n",
    "        return pat_match_with_seg(pattern[1:], saying[1:])\n",
    "    else:\n",
    "        return fail\n",
    "\n",
    "def segment_match(pattern, saying):\n",
    "    seg_pat, rest = pattern[0], pattern[1:]\n",
    "    seg_pat = seg_pat.replace('?*', '?')\n",
    "\n",
    "    if not rest: return (seg_pat, saying), len(saying)    \n",
    "    \n",
    "    for i, token in enumerate(saying):\n",
    "        if rest[0] == token and is_match(rest[1:], saying[(i + 1):]):\n",
    "            return (seg_pat, saying[:i]), i\n",
    "    \n",
    "    return (seg_pat, saying), len(saying)\n",
    "\n",
    "def is_match(rest, saying):\n",
    "    if not rest and not saying:\n",
    "        return True\n",
    "    if not all(a.isalpha() for a in rest[0]):\n",
    "        return True\n",
    "    if rest[0] != saying[0]:\n",
    "        return False\n",
    "    return is_match(rest[1:], saying[1:])    \n",
    "\n",
    "def pat_to_dict(patterns):\n",
    "    return {k: ' '.join(v) if isinstance(v, list) else v for k, v in patterns}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T08:05:54.739282Z",
     "start_time": "2019-04-06T08:05:54.618338Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rule_responses = {\n",
    "    '?*x hello ?*y': ['How do you do', 'Please state your problem'],\n",
    "    '?*x I want ?*y': ['what would it mean if you got ?y', 'Why do you want ?y', 'Suppose you got ?y soon'],\n",
    "    '?*x if ?*y': ['Do you really think its likely that ?y', 'Do you wish that ?y', 'What do you think about ?y', 'Really-- if ?y'],\n",
    "    '?*x no ?*y': ['why not?', 'You are being a negative', 'Are you saying \\'No\\' just to be negative?'],\n",
    "    '?*x I was ?*y': ['Were you really', 'Perhaps I already knew you were ?y', 'Why do you tell me you were ?y now?'],\n",
    "    '?*x I feel ?*y': ['Do you often feel ?y ?', 'What other feelings do you have?'],\n",
    "    '?*x你好?*y': ['你好呀', '请告诉我你的问题'],\n",
    "    '?*x我想?*y': ['你觉得?y有什么意义呢？', '为什么你想?y', '你可以想想你很快就可以?y了'],\n",
    "    '?*x我想要?*y': ['?x想问你，你觉得?y有什么意义呢?', '为什么你想?y', '?x觉得... 你可以想想你很快就可以有?y了', '你看?x像?y不', '我看你就像?y'],\n",
    "    '?*x喜欢?*y': ['喜欢?y的哪里？', '?y有什么好的呢？', '你想要?y吗？'],\n",
    "    '?*x讨厌?*y': ['?y怎么会那么讨厌呢?', '讨厌?y的哪里？', '?y有什么不好呢？', '你不想要?y吗？'],\n",
    "    '?*x AI ?*y': ['你为什么要提AI的事情？', '你为什么觉得AI要解决你的问题？'],\n",
    "    '?*x机器人?*y': ['你为什么要提机器人的事情？', '你为什么觉得机器人要解决你的问题？'],\n",
    "    '?*x对不起?*y': ['不用道歉', '你为什么觉得你需要道歉呢?'],\n",
    "    '?*x我记得?*y': ['你经常会想起这个吗？', '除了?y你还会想起什么吗？', '你为什么和我提起?y'],\n",
    "    '?*x如果?*y': ['你真的觉得?y会发生吗？', '你希望?y吗?', '真的吗？如果?y的话', '关于?y你怎么想？'],\n",
    "    '?*x我?*z梦见?*y':['真的吗? --- ?y', '你在醒着的时候，以前想象过?y吗？', '你以前梦见过?y吗'],\n",
    "    '?*x妈妈?*y': ['你家里除了?y还有谁?', '嗯嗯，多说一点和你家里有关系的', '她对你影响很大吗？'],\n",
    "    '?*x爸爸?*y': ['你家里除了?y还有谁?', '嗯嗯，多说一点和你家里有关系的', '他对你影响很大吗？', '每当你想起你爸爸的时候， 你还会想起其他的吗?'],\n",
    "    '?*x我愿意?*y': ['我可以帮你?y吗？', '你可以解释一下，为什么想?y'],\n",
    "    '?*x我很难过，因为?*y': ['我听到你这么说， 也很难过', '?y不应该让你这么难过的'],\n",
    "    '?*x难过?*y': ['我听到你这么说， 也很难过',\n",
    "                 '不应该让你这么难过的，你觉得你拥有什么，就会不难过?',\n",
    "                 '你觉得事情变成什么样，你就不难过了?'],\n",
    "    '?*x就像?*y': ['你觉得?x和?y有什么相似性？', '?x和?y真的有关系吗？', '怎么说？'],\n",
    "    '?*x和?*y都?*z': ['你觉得?z有什么问题吗?', '?z会对你有什么影响呢?'],\n",
    "    '?*x和?*y一样?*z': ['你觉得?z有什么问题吗?', '?z会对你有什么影响呢?'],\n",
    "    '?*x我是?*y': ['真的吗？', '?x想告诉你，或许我早就知道你是?y', '你为什么现在才告诉我你是?y'],\n",
    "    '?*x我是?*y吗': ['如果你是?y会怎么样呢？', '你觉得你是?y吗', '如果你是?y，那一位着什么?'],\n",
    "    '?*x你是?*y吗':  ['你为什么会对我是不是?y感兴趣?', '那你希望我是?y吗', '你要是喜欢， 我就会是?y'],\n",
    "    '?*x你是?*y' : ['为什么你觉得我是?y'],\n",
    "    '?*x因为?*y' : ['?y是真正的原因吗？', '你觉得会有其他原因吗?'],\n",
    "    '?*x我不能?*y': ['你或许现在就能?*y', '如果你能?*y,会怎样呢？'],\n",
    "    '?*x我觉得?*y': ['你经常这样感觉吗？', '除了到这个，你还有什么其他的感觉吗？'],\n",
    "    '?*x我?*y你?*z': ['其实很有可能我们互相?y'],\n",
    "    '?*x你为什么不?*y': ['你自己为什么不?y', '你觉得我不会?y', '等我心情好了，我就?y'],\n",
    "    '?*x好的?*y': ['好的', '你是一个很正能量的人'],\n",
    "    '?*x嗯嗯?*y': ['好的', '你是一个很正能量的人'],\n",
    "    '?*x不嘛?*y': ['为什么不？', '你有一点负能量', '你说 不，是想表达不想的意思吗？'],\n",
    "    '?*x不要?*y': ['为什么不？', '你有一点负能量', '你说 不，是想表达不想的意思吗？'],\n",
    "    '?*x有些人?*y': ['具体是哪些人呢?'],\n",
    "    '?*x有的人?*y': ['具体是哪些人呢?'],\n",
    "    '?*x某些人?*y': ['具体是哪些人呢?'],\n",
    "    '?*x每个人?*y': ['我确定不是人人都是', '你能想到一点特殊情况吗？', '例如谁？', '你看到的其实只是一小部分人'],\n",
    "    '?*x所有人?*y': ['我确定不是人人都是', '你能想到一点特殊情况吗？', '例如谁？', '你看到的其实只是一小部分人'],\n",
    "    '?*x总是?*y': ['你能想到一些其他情况吗?', '例如什么时候?', '你具体是说哪一次？', '真的---总是吗？'],\n",
    "    '?*x一直?*y': ['你能想到一些其他情况吗?', '例如什么时候?', '你具体是说哪一次？', '真的---总是吗？'],\n",
    "    '?*x或许?*y': ['你看起来不太确定'],\n",
    "    '?*x可能?*y': ['你看起来不太确定'],\n",
    "    '?*x他们是?*y吗？': ['你觉得他们可能不是?y？'],\n",
    "    '?*x': ['很有趣', '请继续', '我不太确定我很理解你说的, 能稍微详细解释一下吗?']\n",
    "}\n",
    "\n",
    "saying= [\n",
    "    'Hello',\n",
    "    \"I think I want a toy\",\n",
    "    \"I was a good boy\",\n",
    "    \"I don't feel so good\",\n",
    "    \"I feel sad\",\n",
    "    \"我和你一样高兴\",\n",
    "    \"我是钢铁侠\",\n",
    "    \"我没出门因为今天下雨\",\n",
    "    \"我觉得我不能现在就放弃\",\n",
    "    \"我喜欢瑞克\",\n",
    "    \"你觉得他们是坏人吗\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T08:05:55.835639Z",
     "start_time": "2019-04-06T08:05:54.741267Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\SHADYR~1\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.889 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello ==> Sorry, I don't know what you're  talking about.\n",
      "I think I want a toy ==> what would it mean if you got a toy\n",
      "I was a good boy ==> Were you really\n",
      "I don't feel so good ==> Sorry, I don't know what you're  talking about.\n",
      "I feel sad ==> What other feelings do you have?\n",
      "我和你一样高兴 ==> 你以前梦见过?y吗\n",
      "我是钢铁侠 ==> 真的吗? --- ?y\n",
      "我没出门因为今天下雨 ==> 你在醒着的时候，以前想象过?y吗？\n",
      "我觉得我不能现在就放弃 ==> 真的吗? --- ?y\n",
      "我喜欢瑞克 ==> ?y有什么好的呢？\n",
      "你觉得他们是坏人吗 ==> 你觉得他们可能不是?y？\n"
     ]
    }
   ],
   "source": [
    "def preprocess(saying, is_pat=True):\n",
    "    if is_pat:\n",
    "        saying_array = jieba.lcut(saying)\n",
    "        res = []\n",
    "        i,m = 0,len(saying_array)\n",
    "        while i < m:\n",
    "            \n",
    "            token = saying_array[i]\n",
    "            if token.isalpha():\n",
    "                res.append(token)\n",
    "                i+=1\n",
    "            elif token == '?' and i < m-1:\n",
    "                j = i+2 if  saying_array[i+1] == '*' else i+1\n",
    "                var = ''.join(saying_array[i:j+1])\n",
    "                res.append(var)\n",
    "                i = j+1\n",
    "            else:\n",
    "                i+=1\n",
    "        return res                                \n",
    "    else:\n",
    "        return [s for s in jieba.lcut(saying) if s.isalpha()]\n",
    "\n",
    "\n",
    "def is_failed_match(matched_pattern, saying_array):\n",
    "    if  matched_pattern == fail:\n",
    "        return True\n",
    "    # if all variables add up equal to  saying_array, the match may be wrong\n",
    "    all_vars = [val for vals in matched_pattern for val in vals[1]]\n",
    "    if all_vars == saying_array:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def get_response(saying, response_rules):\n",
    "    if not isinstance(saying,str) or not isinstance(response_rules, dict):\n",
    "        return nonsense\n",
    "    \n",
    "    for pattern, possible_reponse in response_rules.items():\n",
    "        saying_array = preprocess(saying,False)\n",
    "        pattern_array = preprocess(pattern)\n",
    "        matched_pattern = pat_match_with_seg(pattern_array,saying_array)\n",
    "        # print(matched_pattern)\n",
    "        if not is_failed_match(matched_pattern, saying_array):\n",
    "            res = random.choice(possible_reponse)\n",
    "            return ' '.join(subsitite(res.split(),pat_to_dict(matched_pattern)))\n",
    "    return nonsense\n",
    "\n",
    "\n",
    "def test():\n",
    "    for s in saying:\n",
    "        print(\"{} ==> {}\".format(s,get_response(s,rule_responses)))\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## map search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T08:05:55.857624Z",
     "start_time": "2019-04-06T08:05:55.838637Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BEIJING, CHANGCHUN, MULUMUQI, WUHAN, GUNAGHZOU, SHENZHEN, BANGKOK, SHANGHAI, NEWYORK = \"\"\"\n",
    "BEIJING CHANGCHUN MULUMUQI WUHAN GUANGZHOU SHENZHEN BANGKOK SHANGHAI NEWYORK\n",
    "\"\"\".split()\n",
    "connection = {\n",
    "    CHANGCHUN: [BEIJING],\n",
    "    MULUMUQI: [BEIJING], \n",
    "    BEIJING: [MULUMUQI, CHANGCHUN, WUHAN, SHENZHEN, NEWYORK],\n",
    "    NEWYORK: [BEIJING, SHANGHAI],\n",
    "    SHANGHAI: [NEWYORK, WUHAN],\n",
    "    WUHAN: [SHANGHAI, BEIJING, GUNAGHZOU],\n",
    "    GUNAGHZOU: [WUHAN, BANGKOK],\n",
    "    SHENZHEN: [WUHAN, BANGKOK],\n",
    "    BANGKOK: [SHENZHEN, GUNAGHZOU]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T08:05:56.028544Z",
     "start_time": "2019-04-06T08:05:55.864622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEIJING=>WUHAN=>SHANGHAI\n",
      "BEIJING=>WUHAN\n",
      "BEIJING=>NEWYORK\n"
     ]
    }
   ],
   "source": [
    "def navigator_bfs(start,end,graph):\n",
    "    if start == end:\n",
    "        return [start]\n",
    "    \n",
    "    possible_paths = [[start]]\n",
    "    seen = set()\n",
    "    \n",
    "    while possible_paths:\n",
    "        path = possible_paths.pop(0)\n",
    "        frontier = path[-1]\n",
    "        if frontier not in seen:\n",
    "            \n",
    "            seen.add(frontier)\n",
    "            successors = graph[frontier]\n",
    "            for next_station in successors:\n",
    "                \n",
    "                path.append(next_station)\n",
    "                if next_station == end:\n",
    "                    return path\n",
    "                possible_paths.append([p for p in path])\n",
    "                path.pop()\n",
    "        \n",
    "    return None\n",
    "\n",
    "\n",
    "print ('=>'.join(navigator_bfs(BEIJING,SHANGHAI,connection)))\n",
    "print ('=>'.join(navigator_bfs(BEIJING,WUHAN,connection)))        \n",
    "print('=>'.join(navigator_bfs(BEIJING,NEWYORK,connection)))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T11:50:34.821972Z",
     "start_time": "2019-04-06T11:50:34.788009Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['WUHAN', 'SHANGHAI', 'NEWYORK', 'BEIJING']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def navigator_dfs(start,end,graph):\n",
    "    path = [start]\n",
    "    seen =set({start})\n",
    "    def helper(start,end,graph,path,seen):\n",
    "        if start == end:\n",
    "            return path   \n",
    "        successors = graph[start]\n",
    "        for next_station in successors:\n",
    "            if next_station not in seen:\n",
    "                seen.add(next_station)\n",
    "                path.append(next_station)\n",
    "                if helper(next_station,end,graph,path,seen):\n",
    "                    return path\n",
    "                path.pop()\n",
    "    helper(start,end,graph,path,seen)\n",
    "    return path\n",
    "    \n",
    "\n",
    "# navigator_dfs(BEIJING,SHANGHAI,connection)\n",
    "navigator_dfs(WUHAN,BEIJING,connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gramer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T08:59:58.259824Z",
     "start_time": "2019-04-06T08:59:58.254828Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase \n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => null | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>   蓝色的 |  好看的 | 小小的\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T09:00:29.053494Z",
     "start_time": "2019-04-06T09:00:29.028507Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_grammar(grammar_str, sep='=>'):\n",
    "    grammar = {}\n",
    "    for line in grammar_str.split('\\n'): \n",
    "        line = line.strip()\n",
    "        if not line: continue\n",
    "        \n",
    "        target, rules = line.split(sep)\n",
    "        \n",
    "        grammar[target.strip()] = [r.split() for r in rules.split('|')]\n",
    "    \n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T11:02:57.758240Z",
     "start_time": "2019-04-06T11:02:57.747248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Adj': [['蓝色的'], ['好看的'], ['小小的']],\n",
       " 'Adj*': [['null'], ['Adj', 'Adj*']],\n",
       " 'Article': [['一个'], ['这个']],\n",
       " 'noun': [['女人'], ['篮球'], ['桌子'], ['小猫']],\n",
       " 'noun_phrase': [['Article', 'Adj*', 'noun']],\n",
       " 'sentence': [['noun_phrase', 'verb_phrase']],\n",
       " 'verb': [['看着'], ['坐在'], ['听着'], ['看见']],\n",
       " 'verb_phrase': [['verb', 'noun_phrase']]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = parse_grammar(grammar)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-06T11:04:13.218234Z",
     "start_time": "2019-04-06T11:04:13.201241Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['noun_phrase', 'verb_phrase']\n",
      "['Article', 'Adj*', 'noun']\n",
      "['这个']\n",
      "['Adj', 'Adj*']\n",
      "['小小的']\n",
      "['null']\n",
      "['小猫']\n",
      "['verb', 'noun_phrase']\n",
      "['看着']\n",
      "['Article', 'Adj*', 'noun']\n",
      "['一个']\n",
      "['null']\n",
      "['小猫']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'这个小小的小猫看着一个小猫'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate(grammar,target='sentence'):\n",
    "    if target not in grammar:return target\n",
    "    rule = random.choice(grammar[target])\n",
    "    print(rule)\n",
    "    return ''.join(generate(grammar,r) for r in rule if r != 'null')\n",
    "\n",
    "generate(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
